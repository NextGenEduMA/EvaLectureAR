import os
import google.generativeai as genai
from typing import Dict, List, Tuple
import logging
import json
import re
from difflib import SequenceMatcher
import arabic_reshaper
from bidi.algorithm import get_display

logger = logging.getLogger(__name__)

class ArabicAssessmentEngine:
    def __init__(self):
        self.gemini_model = None
        self._initialize_gemini()

        # Arabic text processing patterns
        self.diacritic_pattern = re.compile(r'[\u064B-\u0652\u0670\u0640]')  # Arabic diacritics
        self.arabic_pattern = re.compile(r'[\u0600-\u06FF\u0750-\u077F]')  # Arabic characters

    def _initialize_gemini(self):
        """Initialize Google Gemini AI"""
        try:
            api_key = os.getenv('GOOGLE_API_KEY')
            if not api_key:
                logger.warning("Google API key not found")
                return

            genai.configure(api_key=api_key)
            # Use the newer model name
            model_name = os.getenv('GEMINI_MODEL', 'gemini-1.5-flash')
            self.gemini_model = genai.GenerativeModel(model_name)
            logger.info(f"Gemini model initialized: {model_name}")

        except Exception as e:
            logger.error(f"Error initializing Gemini: {e}")

    def normalize_arabic_text(self, text: str) -> str:
        """Normalize Arabic text for comparison"""
        if not text:
            return ""

        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text.strip())

        # Normalize Arabic characters
        text = text.replace('Ø£', 'Ø§').replace('Ø¥', 'Ø§').replace('Ø¢', 'Ø§')
        text = text.replace('Ø©', 'Ù‡')
        text = text.replace('Ù‰', 'ÙŠ')

        return text

    def extract_words_without_diacritics(self, text: str) -> List[str]:
        """Extract Arabic words without diacritics"""
        # Remove diacritics
        text_no_diacritics = self.diacritic_pattern.sub('', text)

        # Extract Arabic words
        words = re.findall(self.arabic_pattern, text_no_diacritics)
        return [word for word in words if word.strip()]

    def detect_missing_diacritics(self, original_text: str, transcribed_text: str) -> List[Dict]:
        """Detect missing or incorrect diacritics"""
        errors = []

        try:
            # Remove diacritics from both texts for word alignment
            original_no_diacritics = self.diacritic_pattern.sub('', original_text)
            transcribed_no_diacritics = self.diacritic_pattern.sub('', transcribed_text)

            # Split into words
            original_words = original_no_diacritics.split()
            transcribed_words = transcribed_no_diacritics.split()
            original_with_diacritics = original_text.split()

            # Align words and check for diacritic differences
            for i, (orig_word, trans_word) in enumerate(zip(original_words, transcribed_words)):
                if i < len(original_with_diacritics):
                    orig_with_diac = original_with_diacritics[i]

                    # Check if diacritics are missing
                    if self.diacritic_pattern.search(orig_with_diac) and orig_word == trans_word:
                        errors.append({
                            'type': 'missing_diacritics',
                            'position': i,
                            'expected': orig_with_diac,
                            'actual': trans_word,
                            'severity': 'medium',
                            'message': f'Missing diacritics in word: {orig_with_diac}'
                        })

            return errors

        except Exception as e:
            logger.error(f"Error detecting missing diacritics: {e}")
            return []

    def generate_personalized_assessment(self, student_data: Dict, text_data: Dict,
                                       transcription_result: Dict, errors: List[Dict],
                                       pronunciation_details: Dict = None) -> Dict:
        """Generate comprehensive personalized assessment using AI"""
        try:
            if not self.gemini_model:
                return self._generate_fallback_assessment(student_data, errors)

            # Prepare assessment prompt
            prompt = self._build_assessment_prompt(
                student_data, text_data, transcription_result, errors, pronunciation_details
            )

            # Generate AI assessment
            response = self.gemini_model.generate_content(prompt)
            ai_feedback = response.text

            # Calculate scores
            pronunciation_score = self._calculate_pronunciation_score(pronunciation_details, errors)
            fluency_metrics = self.calculate_fluency_score(
                transcription_result.get('duration', 0),
                len(text_data.get('content', '').split()),
                errors
            )
            accuracy_metrics = self.calculate_accuracy_score(
                text_data.get('content', ''),
                transcription_result.get('transcription', ''),
                errors
            )

            # Calculate overall score with child-friendly weighting
            overall_score = self._calculate_child_friendly_score(
                pronunciation_score, fluency_metrics['fluency_score'],
                accuracy_metrics['accuracy_score'], student_data
            )

            return {
                'overall_score': overall_score,
                'pronunciation_score': pronunciation_score,
                'fluency_score': fluency_metrics['fluency_score'],
                'accuracy_score': accuracy_metrics['accuracy_score'],
                'comprehension_score': self._estimate_comprehension_score(accuracy_metrics, fluency_metrics),
                'words_per_minute': fluency_metrics.get('wpm', 0),
                'reading_duration': transcription_result.get('duration', 0),
                'errors_detected': errors,
                'personalized_feedback': self._format_child_friendly_feedback(ai_feedback, student_data),
                'recommendations': self._generate_recommendations(student_data, errors, overall_score),
                'encouragement': self._generate_encouragement(overall_score, student_data),
                'areas_for_improvement': self._identify_improvement_areas(errors, fluency_metrics, accuracy_metrics),
                'strengths': self._identify_strengths(overall_score, pronunciation_score, fluency_metrics)
            }

        except Exception as e:
            logger.error(f"Error generating personalized assessment: {e}")
            return self._generate_fallback_assessment(student_data, errors)

    def _build_assessment_prompt(self, student_data: Dict, text_data: Dict,
                                transcription_result: Dict, errors: List[Dict],
                                pronunciation_details: Dict = None) -> str:
        """Build comprehensive prompt for AI assessment"""

        grade_level = student_data.get('grade_level', 1)
        difficulty = student_data.get('difficulty_level', 'easy')
        student_name = student_data.get('name', 'Ø§Ù„Ø·Ø§Ù„Ø¨')

        # Check transcription quality
        similarity = SequenceMatcher(None, 
                                   self.normalize_arabic_text(text_data.get('content', '')).split(),
                                   self.normalize_arabic_text(transcription_result.get('transcription', '')).split()).ratio() * 100

        prompt = f"""
Ø£Ù†Øª Ù…Ø¹Ù„Ù… Ù„ØºØ© Ø¹Ø±Ø¨ÙŠØ© Ù…ØªØ®ØµØµ ÙÙŠ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© Ù„Ù„Ø£Ø·ÙØ§Ù„. ÙŠØ±Ø¬Ù‰ ØªÙ‚ÙŠÙŠÙ… Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø·Ø§Ù„Ø¨ Ø§Ù„ØªØ§Ù„ÙŠ:

Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø·Ø§Ù„Ø¨:
- Ø§Ù„Ø§Ø³Ù…: {student_name}
- Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¯Ø±Ø§Ø³ÙŠ: Ø§Ù„ØµÙ {grade_level}
- Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØµØ¹ÙˆØ¨Ø©: {difficulty}

Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ù‚Ø±Ø§Ø¡ØªÙ‡:
"{text_data.get('content', '')}"

Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ Ù‚Ø±Ø£Ù‡ Ø§Ù„Ø·Ø§Ù„Ø¨ ÙØ¹Ù„ÙŠØ§Ù‹:
"{transcription_result.get('transcription', '')}"

Ø¬ÙˆØ¯Ø© Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù…: {'Ø¶Ø¹ÙŠÙØ© - Ù‚Ø¯ ØªÙƒÙˆÙ† Ù‡Ù†Ø§Ùƒ Ø£Ø®Ø·Ø§Ø¡ ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù… ÙˆÙ„ÙŠØ³ Ø§Ù„Ø·Ø§Ù„Ø¨' if similarity < 30 else 'Ø¬ÙŠØ¯Ø©' if similarity > 60 else 'Ù…ØªÙˆØ³Ø·Ø©'}

Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ù…ÙƒØªØ´ÙØ©: {len(errors)} Ø®Ø·Ø£"""

        if similarity < 30:
            prompt += f"""

Ù…Ù„Ø§Ø­Ø¸Ø© Ù…Ù‡Ù…Ø©: Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ·Ø§Ø¨Ù‚ Ù…Ù†Ø®ÙØ¶Ø© Ø¬Ø¯Ø§Ù‹ ({similarity:.1f}%)ØŒ Ù…Ù…Ø§ ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰ Ø§Ø­ØªÙ…Ø§Ù„ ÙˆØ¬ÙˆØ¯ Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù… ÙˆÙ„ÙŠØ³ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø·Ø§Ù„Ø¨. 
ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø§Ù„ØªØ´Ø¬ÙŠØ¹ ÙˆØªÙ‚Ø¯ÙŠÙ… Ù†ØµØ§Ø¦Ø­ Ø¹Ø§Ù…Ø© Ù„ØªØ­Ø³ÙŠÙ† ÙˆØ¶ÙˆØ­ Ø§Ù„Ù†Ø·Ù‚."""

        if errors and len(errors) <= 10:
            prompt += "\nØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡:\n"
            for i, error in enumerate(errors[:5]):  # Limit to 5 errors for brevity
                prompt += f"- {error.get('type', 'Ø®Ø·Ø£')}: {error.get('message', '')}\n"
        elif len(errors) > 10:
            prompt += f"\nØªÙ… Ø§ÙƒØªØ´Ø§Ù {len(errors)} Ø®Ø·Ø£ - Ù‡Ø°Ø§ Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„ÙƒØ¨ÙŠØ± Ù‚Ø¯ ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰ Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù….\n"

        if pronunciation_details:
            prompt += f"\nØ¯Ø±Ø¬Ø© Ø§Ù„Ù†Ø·Ù‚ Ù…Ù† Azure: {pronunciation_details.get('pronunciation_score', 'ØºÙŠØ± Ù…ØªÙˆÙØ±')}\n"

        prompt += f"""
ÙŠØ±Ø¬Ù‰ ØªÙ‚Ø¯ÙŠÙ… ØªÙ‚ÙŠÙŠÙ… Ø´Ø§Ù…Ù„ ÙˆÙ…Ø´Ø¬Ø¹ Ù…Ù†Ø§Ø³Ø¨ Ù„Ø·ÙÙ„ ÙÙŠ Ø§Ù„ØµÙ {grade_level} ÙŠØªØ¶Ù…Ù†:

1. ÙƒÙ„Ù…Ø§Øª ØªØ´Ø¬ÙŠØ¹ ÙˆØ¥ÙŠØ¬Ø§Ø¨ÙŠØ© Ø£ÙˆÙ„Ø§Ù‹
2. Ù†Ù‚Ø§Ø· Ø§Ù„Ù‚ÙˆØ© ÙÙŠ Ø§Ù„Ø£Ø¯Ø§Ø¡ (Ø­ØªÙ‰ Ù„Ùˆ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© ÙÙ‚Ø·)
3. Ù†ØµØ§Ø¦Ø­ Ø¨Ø³ÙŠØ·Ø© ÙˆÙ…Ø­Ø¯Ø¯Ø© Ù„Ù„ØªØ­Ø³ÙŠÙ†
4. ØªØ°ÙƒÙŠØ± Ø¨Ø£Ù† Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙŠØ­Ø³Ù† Ø§Ù„Ø£Ø¯Ø§Ø¡

Ø§Ø¬Ø¹Ù„ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…:
- Ù…Ø´Ø¬Ø¹Ø§Ù‹ Ø¬Ø¯Ø§Ù‹ ÙˆØ¥ÙŠØ¬Ø§Ø¨ÙŠØ§Ù‹
- Ù…Ù†Ø§Ø³Ø¨Ø§Ù‹ Ù„Ø¹Ù…Ø± Ø§Ù„Ø·ÙÙ„
- Ù…Ø­Ø¯Ø¯Ø§Ù‹ ÙˆØ¹Ù…Ù„ÙŠØ§Ù‹
- Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙˆØ§Ø¶Ø­Ø©
"""

        return prompt

    def _calculate_child_friendly_score(self, pronunciation: float, fluency: float,
                                      accuracy: float, student_data: Dict) -> float:
        """Calculate overall score with child-friendly weighting and better handling for poor recognition"""
        grade_level = student_data.get('grade_level', 1)
        difficulty = student_data.get('difficulty_level', 'easy')

        # Adjust weights based on grade level - be more encouraging
        if grade_level <= 2:
            # For younger children, emphasize effort over perfection
            weights = {'accuracy': 0.4, 'pronunciation': 0.3, 'fluency': 0.3}
            base_bonus = 20  # Give 20% bonus for young children
        elif grade_level <= 4:
            # Balanced approach for middle grades
            weights = {'accuracy': 0.4, 'pronunciation': 0.35, 'fluency': 0.25}
            base_bonus = 15  # Give 15% bonus for middle grades
        else:
            # For older children, more balanced but still encouraging
            weights = {'accuracy': 0.35, 'pronunciation': 0.35, 'fluency': 0.3}
            base_bonus = 10  # Give 10% bonus for older children

        # Apply difficulty bonus/penalty (reduced penalties)
        difficulty_modifier = {'easy': 1.0, 'medium': 1.02, 'hard': 1.05}
        modifier = difficulty_modifier.get(difficulty, 1.0)

        # Calculate weighted score
        overall = (
            accuracy * weights['accuracy'] +
            pronunciation * weights['pronunciation'] +
            fluency * weights['fluency']
        ) * modifier

        # Add base bonus for attempting the reading
        overall += base_bonus

        # Ensure minimum encouraging score
        min_score = 25 if grade_level <= 2 else 20 if grade_level <= 4 else 15
        overall = max(min_score, overall)

        return min(100, overall)

    def _format_child_friendly_feedback(self, ai_feedback: str, student_data: Dict) -> str:
        """Format AI feedback to be child-friendly"""
        student_name = student_data.get('name', 'Ø§Ù„Ø·Ø§Ù„Ø¨')

        # Add personal touch and encouragement
        formatted_feedback = f"Ø¹Ø²ÙŠØ²ÙŠ {student_name}ØŒ\n\n"
        formatted_feedback += ai_feedback
        formatted_feedback += f"\n\nØ§Ø³ØªÙ…Ø± ÙÙŠ Ø§Ù„ØªØ¯Ø±Ø¨ ÙˆØ³ØªØµØ¨Ø­ Ø£ÙØ¶Ù„! ğŸŒŸ"

        return formatted_feedback

    def _generate_recommendations(self, student_data: Dict, errors: List[Dict],
                                overall_score: float) -> List[str]:
        """Generate specific recommendations based on performance"""
        recommendations = []
        grade_level = student_data.get('grade_level', 1)

        # Error-based recommendations
        error_types = [error.get('type', '') for error in errors]

        if 'missing_diacritics' in error_types:
            recommendations.append("ØªØ¯Ø±Ø¨ Ø¹Ù„Ù‰ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù†ØµÙˆØµ Ù…Ø¹ Ø§Ù„ØªØ´ÙƒÙŠÙ„ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ø·Ù‚")

        if 'substitution' in error_types or 'mispronunciation' in error_types:
            recommendations.append("Ø§Ø³ØªÙ…Ø¹ Ù„Ù„Ù†Ø·Ù‚ Ø§Ù„ØµØ­ÙŠØ­ Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØµØ¹Ø¨Ø© Ø¹Ø¯Ø© Ù…Ø±Ø§Øª")

        if len(errors) > 5:
            recommendations.append("Ø§Ù‚Ø±Ø£ Ø¨Ø¨Ø·Ø¡ Ø£ÙƒØ«Ø± Ù„Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø¯Ù‚Ø© Ø§Ù„Ù†Ø·Ù‚")

        # Score-based recommendations
        if overall_score < 60:
            recommendations.append("ØªØ¯Ø±Ø¨ Ø¹Ù„Ù‰ Ù†ØµÙˆØµ Ø£Ø³Ù‡Ù„ Ø£ÙˆÙ„Ø§Ù‹ Ù„Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø«Ù‚Ø©")
            recommendations.append("Ø§Ø·Ù„Ø¨ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ù…Ù† Ø§Ù„Ù…Ø¹Ù„Ù… Ø£Ùˆ Ø§Ù„Ø£Ù‡Ù„")
        elif overall_score < 80:
            recommendations.append("Ø£Ø¹Ø¯ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù†Øµ Ø¹Ø¯Ø© Ù…Ø±Ø§Øª Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø·Ù„Ø§Ù‚Ø©")
        else:
            recommendations.append("Ø¬Ø±Ø¨ Ù†ØµÙˆØµØ§Ù‹ Ø£ÙƒØ«Ø± ØµØ¹ÙˆØ¨Ø© Ù„ØªØ·ÙˆÙŠØ± Ù…Ù‡Ø§Ø±Ø§ØªÙƒ")

        return recommendations[:4]  # Limit to 4 recommendations

    def _generate_encouragement(self, overall_score: float, student_data: Dict) -> str:
        """Generate encouraging message based on performance"""
        student_name = student_data.get('name', 'Ø§Ù„Ø·Ø§Ù„Ø¨')

        if overall_score >= 90:
            return f"Ù…Ù…ØªØ§Ø² ÙŠØ§ {student_name}! Ø£Ø¯Ø§Ø¡ Ø±Ø§Ø¦Ø¹ Ø¬Ø¯Ø§Ù‹! ğŸŒŸ"
        elif overall_score >= 80:
            return f"Ø£Ø­Ø³Ù†Øª ÙŠØ§ {student_name}! ØªØ­Ø³Ù† Ù…Ù„Ø­ÙˆØ¸! ğŸ‘"
        elif overall_score >= 70:
            return f"Ø¬ÙŠØ¯ ÙŠØ§ {student_name}! Ø§Ø³ØªÙ…Ø± ÙÙŠ Ø§Ù„ØªØ¯Ø±Ø¨! ğŸ’ª"
        elif overall_score >= 60:
            return f"Ù„Ø§ Ø¨Ø£Ø³ ÙŠØ§ {student_name}ØŒ Ø£Ù†Øª ØªØªØ­Ø³Ù†! ğŸ“ˆ"
        else:
            return f"Ù„Ø§ ØªØ³ØªØ³Ù„Ù… ÙŠØ§ {student_name}ØŒ Ø§Ù„ØªØ¯Ø±Ø¨ Ø³ÙŠØ­Ø³Ù† Ø£Ø¯Ø§Ø¡Ùƒ! ğŸ¯"

    def _identify_improvement_areas(self, errors: List[Dict], fluency_metrics: Dict,
                                  accuracy_metrics: Dict) -> List[str]:
        """Identify specific areas needing improvement"""
        areas = []

        if accuracy_metrics.get('accuracy_score', 0) < 70:
            areas.append("Ø¯Ù‚Ø© Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª")

        if fluency_metrics.get('fluency_score', 0) < 70:
            areas.append("Ø³Ø±Ø¹Ø© Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© ÙˆØ·Ù„Ø§Ù‚ØªÙ‡Ø§")

        error_types = [error.get('type', '') for error in errors]
        if 'missing_diacritics' in error_types:
            areas.append("Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„")

        if any('pronunciation' in error.get('type', '') for error in errors):
            areas.append("ÙˆØ¶ÙˆØ­ Ø§Ù„Ù†Ø·Ù‚")

        return areas

    def _identify_strengths(self, overall_score: float, pronunciation_score: float,
                          fluency_metrics: Dict) -> List[str]:
        """Identify student's strengths"""
        strengths = []

        if pronunciation_score >= 80:
            strengths.append("Ø§Ù„Ù†Ø·Ù‚ Ø§Ù„ÙˆØ§Ø¶Ø­")

        if fluency_metrics.get('fluency_score', 0) >= 80:
            strengths.append("Ø§Ù„Ø·Ù„Ø§Ù‚Ø© ÙÙŠ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©")

        if overall_score >= 80:
            strengths.append("Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø¹Ø§Ù… Ø§Ù„Ù…Ù…ØªØ§Ø²")

        if fluency_metrics.get('pace_rating') == 'normal':
            strengths.append("Ø§Ù„Ø³Ø±Ø¹Ø© Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© ÙÙŠ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©")

        if not strengths:
            strengths.append("Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© ÙˆØ§Ù„Ø¬Ù‡Ø¯ Ø§Ù„Ù…Ø¨Ø°ÙˆÙ„")

        return strengths

    def _generate_fallback_assessment(self, student_data: Dict, errors: List[Dict]) -> Dict:
        """Generate basic assessment when AI is not available"""
        error_count = len(errors)

        # Simple scoring based on error count
        if error_count == 0:
            overall_score = 95
        elif error_count <= 2:
            overall_score = 85
        elif error_count <= 5:
            overall_score = 70
        else:
            overall_score = max(50, 80 - (error_count * 5))

        return {
            'overall_score': overall_score,
            'pronunciation_score': max(60, 100 - (error_count * 10)),
            'fluency_score': 75,
            'accuracy_score': max(50, 100 - (error_count * 8)),
            'comprehension_score': 75,
            'errors_detected': errors,
            'personalized_feedback': f"ØªÙ… Ø±ØµØ¯ {error_count} Ø®Ø·Ø£. Ø§Ø³ØªÙ…Ø± ÙÙŠ Ø§Ù„ØªØ¯Ø±Ø¨ Ù„ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¦Ùƒ!",
            'recommendations': ["ØªØ¯Ø±Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© ÙŠÙˆÙ…ÙŠØ§Ù‹", "Ø§Ø³ØªÙ…Ø¹ Ù„Ù„Ù†Ø·Ù‚ Ø§Ù„ØµØ­ÙŠØ­"],
            'encouragement': "Ø£Ø­Ø³Ù†Øª! Ø§Ø³ØªÙ…Ø± ÙÙŠ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©! ğŸ’ª"
        }
        """Detect various types of pronunciation errors"""
        errors = []

        try:
            # Normalize texts
            orig_normalized = self.normalize_arabic_text(original_text)
            trans_normalized = self.normalize_arabic_text(transcribed_text)

            # Word-level comparison
            orig_words = orig_normalized.split()
            trans_words = trans_normalized.split()

            # Use sequence matcher for alignment
            matcher = SequenceMatcher(None, orig_words, trans_words)

            for tag, i1, i2, j1, j2 in matcher.get_opcodes():
                if tag == 'replace':
                    # Word substitution errors
                    for k in range(max(i2-i1, j2-j1)):
                        orig_idx = i1 + k if i1 + k < i2 else i2 - 1
                        trans_idx = j1 + k if j1 + k < j2 else j2 - 1

                        if orig_idx < len(orig_words) and trans_idx < len(trans_words):
                            errors.append({
                                'type': 'mispronunciation',
                                'position': orig_idx,
                                'expected': orig_words[orig_idx],
                                'actual': trans_words[trans_idx],
                                'severity': self._calculate_error_severity(
                                    orig_words[orig_idx], trans_words[trans_idx]
                                ),
                                'message': f'Mispronounced: {orig_words[orig_idx]} â†’ {trans_words[trans_idx]}'
                            })

                elif tag == 'delete':
                    # Missing words
                    for k in range(i1, i2):
                        errors.append({
                            'type': 'omission',
                            'position': k,
                            'expected': orig_words[k],
                            'actual': '',
                            'severity': 'high',
                            'message': f'Omitted word: {orig_words[k]}'
                        })

                elif tag == 'insert':
                    # Extra words
                    for k in range(j1, j2):
                        errors.append({
                            'type': 'insertion',
                            'position': i1,
                            'expected': '',
                            'actual': trans_words[k],
                            'severity': 'medium',
                            'message': f'Inserted extra word: {trans_words[k]}'
                        })

            # Add Azure pronunciation assessment errors if available
            if pronunciation_details and 'word_details' in pronunciation_details:
                for word_detail in pronunciation_details['word_details']:
                    if word_detail.get('error_type', 'None') != 'None':
                        errors.append({
                            'type': 'pronunciation_accuracy',
                            'position': -1,  # Position not available from Azure
                            'expected': word_detail['word'],
                            'actual': word_detail['word'],
                            'severity': self._map_azure_error_severity(word_detail.get('accuracy_score', 0)),
                            'accuracy_score': word_detail.get('accuracy_score', 0),
                            'error_type': word_detail.get('error_type'),
                            'message': f'Pronunciation issue with: {word_detail["word"]} (Score: {word_detail.get("accuracy_score", 0)})'
                        })

            return errors

        except Exception as e:
            logger.error(f"Error detecting pronunciation errors: {e}")
            return []

    def _calculate_error_severity(self, expected: str, actual: str) -> str:
        """Calculate error severity based on word similarity"""
        similarity = SequenceMatcher(None, expected, actual).ratio()

        if similarity > 0.8:
            return 'low'
        elif similarity > 0.5:
            return 'medium'
        else:
            return 'high'

    def _map_azure_error_severity(self, accuracy_score: float) -> str:
        """Map Azure accuracy score to severity level"""
        if accuracy_score >= 80:
            return 'low'
        elif accuracy_score >= 60:
            return 'medium'
        else:
            return 'high'

    def calculate_fluency_score(self, audio_duration: float, word_count: int,
                              errors: List[Dict]) -> Dict:
        """Calculate fluency metrics"""
        try:
            if audio_duration <= 0 or word_count <= 0:
                return {'wpm': 0, 'fluency_score': 0, 'pace_rating': 'unknown'}

            # Calculate words per minute
            wpm = (word_count / audio_duration) * 60

            # Adjust for errors (each error reduces fluency)
            error_penalty = len(errors) * 2  # 2 WPM penalty per error
            adjusted_wpm = max(0, wpm - error_penalty)

            # Calculate fluency score (0-100)
            # Optimal Arabic reading speed: 120-180 WPM
            optimal_wpm = 150
            if adjusted_wpm <= optimal_wpm:
                fluency_score = (adjusted_wpm / optimal_wpm) * 100
            else:
                # Penalty for reading too fast
                fluency_score = max(0, 100 - ((adjusted_wpm - optimal_wpm) / optimal_wpm) * 50)

            # Determine pace rating
            if wpm < 80:
                pace_rating = 'slow'
            elif wpm > 200:
                pace_rating = 'fast'
            else:
                pace_rating = 'normal'

            return {
                'wpm': round(wpm, 2),
                'adjusted_wpm': round(adjusted_wpm, 2),
                'fluency_score': round(fluency_score, 2),
                'pace_rating': pace_rating,
                'error_count': len(errors)
            }

        except Exception as e:
            logger.error(f"Error calculating fluency score: {e}")
            return {'wpm': 0, 'fluency_score': 0, 'pace_rating': 'unknown'}

    def calculate_accuracy_score(self, original_text: str, transcribed_text: str,
                               errors: List[Dict]) -> Dict:
        """Calculate reading accuracy score with improved handling for poor transcriptions"""
        try:
            orig_words = self.normalize_arabic_text(original_text).split()
            trans_words = self.normalize_arabic_text(transcribed_text).split()
            total_words = len(orig_words)

            if total_words == 0:
                return {'accuracy_score': 0, 'error_rate': 100, 'similarity_score': 0}

            # Calculate text similarity using SequenceMatcher
            from difflib import SequenceMatcher
            similarity = SequenceMatcher(None, orig_words, trans_words).ratio() * 100

            # If similarity is very low (< 30%), the speech recognition likely failed
            # In this case, be more lenient with scoring
            if similarity < 30:
                logger.warning(f"Very low similarity ({similarity:.1f}%) - possible speech recognition error")
                # Give partial credit for attempted reading
                base_score = 40  # Base score for attempting to read
                return {
                    'accuracy_score': base_score,
                    'similarity_score': round(similarity, 2),
                    'error_rate': round((len(errors) / total_words) * 100, 2),
                    'total_words': total_words,
                    'total_errors': len(errors),
                    'error_breakdown': {
                        'mispronunciation': len(errors),
                        'omission': 0,
                        'insertion': 0,
                        'missing_diacritics': 0,
                        'pronunciation_accuracy': 0
                    },
                    'transcription_quality': 'poor'
                }

            # Count different types of errors
            error_counts = {
                'mispronunciation': 0,
                'omission': 0,
                'insertion': 0,
                'missing_diacritics': 0,
                'pronunciation_accuracy': 0
            }

            for error in errors:
                error_type = error.get('type', 'unknown')
                if error_type in error_counts:
                    error_counts[error_type] += 1

            # Calculate weighted error score with more lenient weights
            error_weights = {
                'mispronunciation': 0.7,  # Reduced from 1.0
                'omission': 1.0,          # Reduced from 1.5
                'insertion': 0.3,         # Reduced from 0.5
                'missing_diacritics': 0.2, # Reduced from 0.3
                'pronunciation_accuracy': 0.5  # Reduced from 0.8
            }

            weighted_errors = sum(
                error_counts[error_type] * error_weights.get(error_type, 1.0)
                for error_type in error_counts
            )

            # More lenient error penalty calculation
            error_penalty = min(60, (weighted_errors / total_words) * 80)  # Cap penalty at 60%
            accuracy_score = max(20, similarity - error_penalty)  # Minimum 20% for attempt

            # If we have no errors but low similarity, use similarity score
            if len(errors) == 0 and similarity > accuracy_score:
                accuracy_score = similarity

            error_rate = (len(errors) / total_words) * 100

            return {
                'accuracy_score': round(accuracy_score, 2),
                'similarity_score': round(similarity, 2),
                'error_rate': round(error_rate, 2),
                'total_words': total_words,
                'total_errors': len(errors),
                'error_breakdown': error_counts,
                'transcription_quality': 'good' if similarity > 60 else 'fair' if similarity > 30 else 'poor'
            }

        except Exception as e:
            logger.error(f"Error calculating accuracy score: {e}")
            return {'accuracy_score': 0, 'error_rate': 100, 'similarity_score': 0}

    def generate_comprehensive_assessment(self, original_text: str, transcribed_text: str,
                                        audio_duration: float, pronunciation_details: Dict = None) -> Dict:
        """Generate comprehensive reading assessment"""
        try:
            # Handle empty transcription
            if not transcribed_text or not transcribed_text.strip():
                logger.warning("Empty transcribed text - generating minimal assessment")
                return {
                    'overall_score': 0,
                    'pronunciation_score': 0,
                    'fluency_score': 0,
                    'accuracy_score': 0,
                    'comprehension_score': 0,
                    'detailed_metrics': {
                        'fluency': {'fluency_score': 0, 'words_per_minute': 0},
                        'accuracy': {'accuracy_score': 0, 'similarity_score': 0},
                        'pronunciation': pronunciation_details or {}
                    },
                    'errors': [],
                    'error_summary': 'Ù„Ù… ÙŠØªÙ… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù… Ø¨ÙˆØ¶ÙˆØ­',
                    'feedback': 'Ù„Ù… ÙŠØªÙ… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù… Ø¨Ø´ÙƒÙ„ ÙˆØ§Ø¶Ø­. ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¶ÙˆØ­ Ø§Ù„Ù†Ø·Ù‚ ÙˆØ§Ù„ØªØ³Ø¬ÙŠÙ„ ÙÙŠ Ù…ÙƒØ§Ù† Ù‡Ø§Ø¯Ø¦.',
                    'recommendations': [
                        'ØªØ­Ø¯Ø« Ø¨ÙˆØ¶ÙˆØ­ Ø£ÙƒØ¨Ø±',
                        'ØªØ£ÙƒØ¯ Ù…Ù† Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ø¶ÙˆØ¶Ø§Ø¡ ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©',
                        'Ø§Ù‚ØªØ±Ø¨ Ù…Ù† Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†',
                        'ØªØ­Ø¯Ø« Ø¨ØµÙˆØª Ø£Ø¹Ù„Ù‰ Ù‚Ù„ÙŠÙ„Ø§Ù‹'
                    ]
                }

            # Detect errors
            pronunciation_errors = self.detect_pronunciation_errors(
                original_text, transcribed_text, pronunciation_details
            )
            diacritic_errors = self.detect_missing_diacritics(original_text, transcribed_text)

            all_errors = pronunciation_errors + diacritic_errors

            # Calculate metrics
            word_count = len(self.normalize_arabic_text(original_text).split())
            fluency_metrics = self.calculate_fluency_score(audio_duration, word_count, all_errors)
            accuracy_metrics = self.calculate_accuracy_score(original_text, transcribed_text, all_errors)

            # Calculate overall score
            pronunciation_score = pronunciation_details.get('pronunciation_score', 0) if pronunciation_details else 0

            # If pronunciation score is 0 but we have transcription, use accuracy as pronunciation indicator
            if pronunciation_score == 0 and transcribed_text.strip():
                pronunciation_score = accuracy_metrics['accuracy_score']

            overall_score = (
                fluency_metrics['fluency_score'] * 0.3 +
                accuracy_metrics['accuracy_score'] * 0.4 +
                pronunciation_score * 0.3
            )

            # Generate AI feedback using Gemini
            ai_feedback = self.generate_ai_feedback(
                original_text, transcribed_text, all_errors,
                fluency_metrics, accuracy_metrics
            )

            return {
                'overall_score': round(overall_score, 2),
                'pronunciation_score': round(pronunciation_score, 2),
                'fluency_score': fluency_metrics['fluency_score'],
                'accuracy_score': accuracy_metrics['accuracy_score'],
                'comprehension_score': self._estimate_comprehension_score(accuracy_metrics['accuracy_score']),

                'detailed_metrics': {
                    'fluency': fluency_metrics,
                    'accuracy': accuracy_metrics,
                    'pronunciation': pronunciation_details or {}
                },

                'errors': all_errors,
                'error_summary': self._summarize_errors(all_errors),

                'feedback': ai_feedback,
                'recommendations': self._generate_recommendations(all_errors, fluency_metrics, accuracy_metrics)
            }

        except Exception as e:
            logger.error(f"Error generating comprehensive assessment: {e}")
            return {
                'overall_score': 0,
                'pronunciation_score': 0,
                'fluency_score': 0,
                'accuracy_score': 0,
                'comprehension_score': 0,
                'error': str(e),
                'feedback': 'Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªÙ‚ÙŠÙŠÙ…. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.',
                'recommendations': ['ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰']
            }

    def generate_ai_feedback(self, original_text: str, transcribed_text: str,
                           errors: List[Dict], fluency_metrics: Dict, accuracy_metrics: Dict) -> str:
        """Generate personalized feedback using Gemini AI"""
        if not self.gemini_model:
            return self._generate_basic_feedback(errors, fluency_metrics, accuracy_metrics)

        try:
            # Prepare prompt for Gemini
            prompt = f"""
            You are an Arabic language teacher providing brief feedback on a student's reading.

            Original text: {original_text}
            Student read: {transcribed_text}

            Performance:
            - Fluency: {fluency_metrics.get('fluency_score', 0)}/100
            - Accuracy: {accuracy_metrics.get('accuracy_score', 0)}/100
            - Reading speed: {fluency_metrics.get('wpm', 0)} words/min
            - Errors found: {len(errors)}

            Main errors: {[error.get('expected', '') + ' â†’ ' + error.get('actual', '') for error in errors[:3]]}

            Provide feedback in EXACTLY 4 lines maximum:
            Line 1: One sentence acknowledging their performance
            Line 2: One specific area to improve (based on main errors)
            Line 3: One practical tip for improvement
            Line 4: One encouraging closing statement

            Keep it simple, specific, and encouraging. No more than 4 lines total.
            Write in Arabic only.
            """

            response = self.gemini_model.generate_content(prompt)
            return response.text

        except Exception as e:
            logger.error(f"Error generating AI feedback: {e}")
            return self._generate_basic_feedback(errors, fluency_metrics, accuracy_metrics)

    def _generate_basic_feedback(self, errors: List[Dict], fluency_metrics: Dict, accuracy_metrics: Dict) -> str:
        """Generate basic feedback when AI is not available"""
        accuracy_score = accuracy_metrics.get('accuracy_score', 0)
        fluency_score = fluency_metrics.get('fluency_score', 0)
        error_count = len(errors)

        # Generate short, specific feedback
        if accuracy_score >= 80:
            performance_msg = "Ø£Ø¯Ø§Ø¡ Ø¬ÙŠØ¯ ÙÙŠ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©!"
        elif accuracy_score >= 60:
            performance_msg = "Ø£Ø¯Ø§Ø¡ Ù…Ù‚Ø¨ÙˆÙ„ Ù…Ø¹ ÙˆØ¬ÙˆØ¯ Ø¨Ø¹Ø¶ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡."
        else:
            performance_msg = "ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ­Ø³ÙŠÙ† Ø¯Ù‚Ø© Ø§Ù„Ù†Ø·Ù‚."

        if error_count > 0:
            main_errors = [error.get('expected', '') for error in errors[:2]]
            improvement_msg = f"Ø±ÙƒØ² Ø¹Ù„Ù‰ Ù†Ø·Ù‚: {', '.join(main_errors)}"
        else:
            improvement_msg = "Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù…Ù…ØªØ§Ø²."

        tip_msg = "Ø§Ø³ØªÙ…Ø¹ Ù„Ù„Ù†Øµ ÙˆØ§Ù‚Ø±Ø£ Ø¨Ø¨Ø·Ø¡ Ø£ÙƒØ«Ø±."
        encouragement_msg = "Ø§Ø³ØªÙ…Ø± ÙÙŠ Ø§Ù„ØªØ¯Ø±Ø¨ ÙˆØ³ØªØªØ­Ø³Ù†! ğŸ’ª"

        return f"{performance_msg}\n{improvement_msg}\n{tip_msg}\n{encouragement_msg}"

    def _estimate_comprehension_score(self, accuracy_score: float) -> float:
        """Estimate comprehension score based on accuracy"""
        # Simple heuristic: comprehension correlates with accuracy
        return min(100, accuracy_score * 1.1)

    def _summarize_errors(self, errors: List[Dict]) -> Dict:
        """Summarize errors by type and severity"""
        summary = {
            'total_errors': len(errors),
            'by_type': {},
            'by_severity': {'low': 0, 'medium': 0, 'high': 0}
        }

        for error in errors:
            error_type = error.get('type', 'unknown')
            severity = error.get('severity', 'medium')

            summary['by_type'][error_type] = summary['by_type'].get(error_type, 0) + 1
            summary['by_severity'][severity] += 1

        return summary

    def _generate_recommendations(self, errors: List[Dict], fluency_metrics: Dict,
                                accuracy_metrics: Dict) -> List[str]:
        """Generate specific improvement recommendations"""
        recommendations = []

        # Fluency recommendations
        wpm = fluency_metrics.get('wpm', 0)
        if wpm < 80:
            recommendations.append("Practice reading aloud daily to improve reading speed")
        elif wpm > 200:
            recommendations.append("Slow down your reading pace to improve accuracy")

        # Accuracy recommendations
        accuracy_score = accuracy_metrics.get('accuracy_score', 0)
        if accuracy_score < 70:
            recommendations.append("Focus on pronunciation drills for difficult Arabic sounds")

        # Error-specific recommendations
        error_types = set(error.get('type') for error in errors)

        if 'missing_diacritics' in error_types:
            recommendations.append("Study Arabic texts with full diacritics (ØªØ´ÙƒÙŠÙ„)")
            recommendations.append("Practice vowel sounds and their proper pronunciation")

        if 'mispronunciation' in error_types:
            recommendations.append("Use audio resources to hear correct pronunciation")
            recommendations.append("Practice with native Arabic speakers when possible")

        if 'omission' in error_types:
            recommendations.append("Read more slowly and carefully to avoid skipping words")

        # General recommendations
        if len(errors) > 5:
            recommendations.append("Break down complex texts into smaller sections")
            recommendations.append("Record yourself reading and compare with native speakers")

        return recommendations[:5]  # Limit to top 5 recommendations

    def detect_pronunciation_errors(self, original_text: str, transcribed_text: str,
                                  pronunciation_details: Dict = None) -> List[Dict]:
        """Detect pronunciation errors by comparing original and transcribed text"""
        errors = []

        try:
            if not transcribed_text or not transcribed_text.strip():
                return [{
                    'type': 'no_transcription',
                    'severity': 'high',
                    'message': 'Ù„Ù… ÙŠØªÙ… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù…',
                    'position': 0
                }]

            # Normalize both texts for comparison
            orig_normalized = self.normalize_arabic_text(original_text)
            trans_normalized = self.normalize_arabic_text(transcribed_text)

            orig_words = orig_normalized.split()
            trans_words = trans_normalized.split()

            # Use pronunciation details if available (Azure Speech Services)
            if pronunciation_details and not pronunciation_details.get('error'):
                word_details = pronunciation_details.get('word_details', [])

                for word_detail in word_details:
                    accuracy_score = word_detail.get('accuracy_score', 0)
                    word = word_detail.get('word', '')
                    error_type = word_detail.get('error_type', '')

                    # Only flag as error if accuracy is very low AND error type is significant
                    if accuracy_score < 40 and error_type in ['Omission', 'Mispronunciation']:
                        errors.append({
                            'type': 'mispronunciation',
                            'severity': 'high' if accuracy_score < 20 else 'medium',
                            'word': word,
                            'accuracy_score': accuracy_score,
                            'message': f'Ù†Ø·Ù‚ ØºÙŠØ± Ø¯Ù‚ÙŠÙ‚ Ù„Ù„ÙƒÙ„Ù…Ø©: {word}',
                            'suggestion': 'Ø­Ø§ÙˆÙ„ Ø§Ù„Ù†Ø·Ù‚ Ø¨ÙˆØ¶ÙˆØ­ Ø£ÙƒØ¨Ø±'
                        })
                    elif accuracy_score < 60 and error_type == 'Omission':
                        # Be more lenient with omissions as they might be recognition errors
                        errors.append({
                            'type': 'mispronunciation',
                            'severity': 'medium',
                            'word': word,
                            'accuracy_score': accuracy_score,
                            'message': f'Ù†Ø·Ù‚ ØºÙŠØ± Ø¯Ù‚ÙŠÙ‚ Ù„Ù„ÙƒÙ„Ù…Ø©: {word}',
                            'suggestion': 'Ø­Ø§ÙˆÙ„ Ø§Ù„Ù†Ø·Ù‚ Ø¨ÙˆØ¶ÙˆØ­ Ø£ÙƒØ¨Ø±'
                        })

            # Check overall similarity first - if very low, limit detailed word comparison
            # as it's likely a speech recognition issue rather than reading errors
            overall_similarity = SequenceMatcher(None, orig_words, trans_words).ratio() * 100
            
            # Only do detailed word-by-word comparison if similarity is reasonable
            if overall_similarity > 25:  # Only if some similarity exists
                # Compare word-by-word using sequence matching
                from difflib import SequenceMatcher
                matcher = SequenceMatcher(None, orig_words, trans_words)

                error_limit = min(10, len(orig_words))  # Limit errors to prevent overwhelming feedback

                for tag, i1, i2, j1, j2 in matcher.get_opcodes():
                    if len(errors) >= error_limit:  # Stop if we have too many errors already
                        break
                        
                    if tag == 'delete' and len(errors) < error_limit - 2:  # Missing words (omissions)
                        for i in range(i1, min(i2, i1 + 3)):  # Limit to 3 omissions per sequence
                            if len(errors) < error_limit:
                                errors.append({
                                    'type': 'mispronunciation',  # Treat as mispronunciation instead of omission
                                    'severity': 'high',
                                    'expected': orig_words[i],
                                    'actual': 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯',
                                    'position': i,
                                    'message': f'Ù†Ø·Ù‚ Ø®Ø§Ø·Ø¦: Ù‚ÙŠÙ„ "ØºÙŠØ± Ù…Ø­Ø¯Ø¯" Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† "{orig_words[i]}"',
                                    'suggestion': f'Ø§Ù„Ù†Ø·Ù‚ Ø§Ù„ØµØ­ÙŠØ­ Ù‡Ùˆ: {orig_words[i]}'
                                })

                    elif tag == 'replace':  # Substituted words (mispronunciations)
                        for i, j in zip(range(i1, min(i2, i1 + 3)), range(j1, min(j2, j1 + 3))):
                            if len(errors) < error_limit and i < len(orig_words) and j < len(trans_words):
                                errors.append({
                                    'type': 'mispronunciation',
                                    'severity': 'high',
                                    'expected': orig_words[i],
                                    'actual': trans_words[j],
                                    'position': i,
                                    'message': f'Ù†Ø·Ù‚ Ø®Ø§Ø·Ø¦: Ù‚ÙŠÙ„ "{trans_words[j]}" Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† "{orig_words[i]}"',
                                    'suggestion': f'Ø§Ù„Ù†Ø·Ù‚ Ø§Ù„ØµØ­ÙŠØ­ Ù‡Ùˆ: {orig_words[i]}'
                                })
            else:
                # Very low similarity - likely speech recognition issue
                # Add only a few generic errors to acknowledge the issue without overwhelming
                logger.warning(f"Very low similarity ({overall_similarity:.1f}%) in pronunciation error detection")
                for i, word in enumerate(orig_words[:5]):  # Only first 5 words
                    errors.append({
                        'type': 'mispronunciation',
                        'severity': 'medium',  # Lower severity for recognition issues
                        'expected': word,
                        'actual': 'ØºÙŠØ± ÙˆØ§Ø¶Ø­',
                        'position': i,
                        'message': f'Ù„Ù… ÙŠØªÙ… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ù†Ø·Ù‚ Ø§Ù„ØµØ­ÙŠØ­ Ù„Ù„ÙƒÙ„Ù…Ø©: {word}',
                        'suggestion': f'Ø­Ø§ÙˆÙ„ Ù†Ø·Ù‚ Ø§Ù„ÙƒÙ„Ù…Ø© "{word}" Ø¨ÙˆØ¶ÙˆØ­ Ø£ÙƒØ¨Ø±'
                    })

            return errors

        except Exception as e:
            logger.error(f"Error detecting pronunciation errors: {e}")
            return []

    # ...existing code...
